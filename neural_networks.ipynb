{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a neural network from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Activation function will be the relu function throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_fun(z):\n",
    "    a = max(0, z)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural_network(layer_sizes) takes a list/ nparray which has the number of neurons it wants in the ith layer as its ith element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(layer_sizes):\n",
    "    params = {} #an empty dictionary\n",
    "\n",
    "    for i in range (1, len(layer_sizes)):\n",
    "        params['W' + str(i)] = np.random.rand(layer_sizes[i], layer_sizes[i-1])*0.01\n",
    "        params['B' + str(i)] = np.random.rand(layer_sizes[i]).reshape(layer_sizes[i], 1)*0.01\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward Propogation: Going from ith to (i+1)th layer; i.e., calculating A(i+1) using A(i), W(i) and B(i)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propogation(X_train, params):\n",
    "    layers=len(params)//2\n",
    "    values = {}\n",
    "\n",
    "    for i in range(1, layers+1):\n",
    "        if i == 1:\n",
    "            values['Z' + str(i)] = params['W'+str(i)]@X_train + params['B'+str(i)]\n",
    "            values['A' + str(i)] = relu_fun(values['Z' + str(i)])\n",
    "        else:\n",
    "            values['Z' + str(i)] = params['W'+str(i)]@values['A' + str(i-1)] + params['B'+str(i)]\n",
    "            if i==layers:\n",
    "                values['A' + str(i)] = values['Z' + str(i)]\n",
    "            else:\n",
    "                values['A' + str(i)] = relu_fun(values['Z' + str(i)])\n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compute_cost(values, Y_train) function to find the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(values, Y_train):\n",
    "    #mean square error = (1/2*m)(summation((y_pred-y_actual)^2))\n",
    "    y_pred = values['A' + str(len(values)/2)]\n",
    "    cost = (np.sum(np.square(y_pred-Y_train)))/2/len(Y_train)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we find all the gradients using backward propogation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def backward_propagation(params, values, X_train, Y_train): #takes parameters, activations, training set as input and returns gradients wrt parameters\n",
    "    layers = len(params)//2\n",
    "    m = len(Y_train)\n",
    "    grads = {}\n",
    "    for i in range(layers,0,-1):\n",
    "        if i==layers:\n",
    "            dA = (1/m )* (values['A' + str(i)] - Y_train)\n",
    "            dZ = dA\n",
    "        else:\n",
    "            dA = np.dot(params['W' + str(i+1)].T, dZ)\n",
    "            dZ = np.multiply(dA, np.where(values['A' + str(i)]>=0, 1, 0))\n",
    "        if i==1:\n",
    "            grads['W' + str(i)] = 1/m * np.dot(dZ, X_train.T)\n",
    "            grads['B' + str(i)] = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "        else:\n",
    "            grads['W' + str(i)] = 1/m * np.dot(dZ,values['A' + str(i-1)].T)\n",
    "            grads['B' + str(i)] = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "updating_params uses the gradients calculated and the initial values of the various parameters to find their final value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updating_params(params, learning_rate, grads):\n",
    "    updated_params = {}\n",
    "    \n",
    "    for i in range(1, len(params)//2+1):\n",
    "        updated_params['W' + str(i)] = params['W' + str(i)] - learning_rate*grads['W' + str(i)]\n",
    "        updated_params['B' + str(i)] = params['B' + str(i)] - learning_rate*grads['B' + str(i)]\n",
    "    \n",
    "    return updated_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now we have completed the work of any neural network. We just have to stitch it all together so that the user gives an input of training data, and the hyperparameters, and we can perform all these functions in the right order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X_train, Y_train, learning_rate, layer_sizes, num_iters):\n",
    "    params = neural_network(layer_sizes)\n",
    "    for i in range(num_iters):\n",
    "        values = forward_propogation(X_train, params)\n",
    "        grads = backward_propagation(params, values, X_train, Y_train)\n",
    "        \n",
    "        params = updating_params(params, learning_rate, grads)\n",
    "        print(\"iteration :\", i+1)\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to find the accuracy and another to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(X_train, X_test, Y_train, Y_test, params):\n",
    "    values_train=forward_propogation(X_train, params)\n",
    "    values_test=forward_propogation(X_test, params)\n",
    "    train_acc=np.sqrt(mean_squared_error(Y_train, values_train['A'+str(len(params)//2)].T))\n",
    "    test_acc=np.sqrt(mean_squared_error(Y_test, values_test['A'+str(len(params)//2)].T))\n",
    "    return train_acc, test_acc\n",
    "\n",
    "def predict(X_test, params):\n",
    "    values=forward_propogation(X_test, params, relu_fun)\n",
    "    pred=values['A'+str(len(params)//2)].T\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the Boston Housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_boston=pd.read_csv('database/BostonHousing.csv')\n",
    "df_boston_X=df_boston.drop(['medv'], axis=1)\n",
    "df_boston_Y=df_boston['medv']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test=train_test_split(df_boston_X, df_boston_Y, test_size=0.2, random_state=4)\n",
    "X_train=X_train.T\n",
    "X_test=X_test.T\n",
    "Y_train=Y_train.T\n",
    "Y_test=Y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 1\n",
      "iteration : 2\n",
      "iteration : 3\n",
      "iteration : 4\n",
      "iteration : 5\n",
      "iteration : 6\n",
      "iteration : 7\n",
      "iteration : 8\n",
      "iteration : 9\n",
      "iteration : 10\n",
      "iteration : 11\n",
      "iteration : 12\n",
      "iteration : 13\n",
      "iteration : 14\n",
      "iteration : 15\n",
      "iteration : 16\n",
      "iteration : 17\n",
      "iteration : 18\n",
      "iteration : 19\n",
      "iteration : 20\n",
      "iteration : 21\n",
      "iteration : 22\n",
      "iteration : 23\n",
      "iteration : 24\n",
      "iteration : 25\n",
      "iteration : 26\n",
      "iteration : 27\n",
      "iteration : 28\n",
      "iteration : 29\n",
      "iteration : 30\n",
      "iteration : 31\n",
      "iteration : 32\n",
      "iteration : 33\n",
      "iteration : 34\n",
      "iteration : 35\n",
      "iteration : 36\n",
      "iteration : 37\n",
      "iteration : 38\n",
      "iteration : 39\n",
      "iteration : 40\n",
      "iteration : 41\n",
      "iteration : 42\n",
      "iteration : 43\n",
      "iteration : 44\n",
      "iteration : 45\n",
      "iteration : 46\n",
      "iteration : 47\n",
      "iteration : 48\n",
      "iteration : 49\n",
      "iteration : 50\n",
      "iteration : 51\n",
      "iteration : 52\n",
      "iteration : 53\n",
      "iteration : 54\n",
      "iteration : 55\n",
      "iteration : 56\n",
      "iteration : 57\n",
      "iteration : 58\n",
      "iteration : 59\n",
      "iteration : 60\n",
      "iteration : 61\n",
      "iteration : 62\n",
      "iteration : 63\n",
      "iteration : 64\n",
      "iteration : 65\n",
      "iteration : 66\n",
      "iteration : 67\n",
      "iteration : 68\n",
      "iteration : 69\n",
      "iteration : 70\n",
      "iteration : 71\n",
      "iteration : 72\n",
      "iteration : 73\n",
      "iteration : 74\n",
      "iteration : 75\n",
      "iteration : 76\n",
      "iteration : 77\n",
      "iteration : 78\n",
      "iteration : 79\n",
      "iteration : 80\n",
      "iteration : 81\n",
      "iteration : 82\n",
      "iteration : 83\n",
      "iteration : 84\n",
      "iteration : 85\n",
      "iteration : 86\n",
      "iteration : 87\n",
      "iteration : 88\n",
      "iteration : 89\n",
      "iteration : 90\n",
      "iteration : 91\n",
      "iteration : 92\n",
      "iteration : 93\n",
      "iteration : 94\n",
      "iteration : 95\n",
      "iteration : 96\n",
      "iteration : 97\n",
      "iteration : 98\n",
      "iteration : 99\n",
      "iteration : 100\n"
     ]
    }
   ],
   "source": [
    "arr_X_train=X_train.values\n",
    "arr_X_test=X_test.values\n",
    "arr_Y_train=Y_train.values\n",
    "arr_Y_test=Y_test.values\n",
    "\n",
    "layer_sizes=[13, 64, 128, 64, 1]\n",
    "relu_fun=np.vectorize(relu_fun)\n",
    "params=fit(arr_X_train, arr_Y_train, 0.01, layer_sizes, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.776763095703163 9.911634095608784\n"
     ]
    }
   ],
   "source": [
    "train_acc, test_acc=compute_accuracy(arr_X_train, arr_X_test, arr_Y_train, arr_Y_test, params)\n",
    "print(train_acc, test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
